ditto {
  mapping-strategy.implementation = "org.eclipse.ditto.services.models.thingsearch.ThingSearchMappingStrategies"
  cluster-downing.role = "things-search"

  persistence.operations.delay-after-persistence-actor-shutdown = 5s
  persistence.operations.delay-after-persistence-actor-shutdown = ${?DELAY_AFTER_PERSISTENCE_ACTOR_SHUTDOWN}

  mongodb {
    pool {
      maxSize = 1000
      maxSize = ${?MONGO_DB_CONNECTION_POOL_SIZE}
      maxWaitTime = 30s
      maxWaitTime = ${?MONGO_DB_CONNECTION_POOL_WAIT_TIME}
      maxWaitQueueSize = 1000
      maxWaitQueueSize = ${?MONGO_DB_CONNECTION_POOL_QUEUE_SIZE}
      jmxListenerEnabled = false
      jmxListenerEnabled = ${?MONGO_DB_CONNECTION_POOL_JMX_LISTENER_ENABLED}
    }

    breaker {
      maxFailures = 5 # defines ater how many failures the circuit breaker should open
      maxFailures = ${?BREAKER_MAXFAILURES}
      timeout {
        call = 5s # MongoDB Timeouts causing the circuitBreaker to open - "0s" disables timeouts opening the breaker
        call = ${?BREAKER_TIMEOUT}
        reset = 10s # after this time in "Open" state, the cicuitBreaker is "Half-opened" again
        reset = ${?BREAKER_RESET}
      }
    }

    monitoring {
      commands = true
      commands = ${?MONGODB_MONITORING_COMMANDS_ENABLED}
      connection-pool = true
      commands = ${?MONGODB_MONITORING_CONNECTION_POOL_ENABLED}
    }

    options {
      ssl = false
      ssl = ${?MONGO_DB_SSL_ENABLED}
      w = 1
    }
  }

  things-search {
    mongo-hints-by-namespace = ${?MONGO_HINTS_BY_NAMESPACE}

    delete {
      event = true
      event = ${?THINGS_SEARCH_DELETE_EVENT}
      namespace = true
      namespace = ${?THINGS_SEARCH_DELETE_NAMESPACE}
    }

    index-initialization {
      #indices should be created within this application
      enabled = true
      enabled = ${?INDEX_INITIALIZATION_ENABLED}
    }

    # configuration regarding physical deletion of "__deleted" Things from "thingEntities" collection
    deletion {
      enabled = true

      # delete "__deleted" Things older than:
      deletion-age = 3d
      # run each:
      run-interval = 24h
      # the first run should be at (UTC time):
      first-interval-hour = 21 # 21:00 UTC
    }

    updater {
      max-idle-time = 25h
      max-idle-time = ${?ACTIVITY_CHECK_INTERVAL}

      event-processing-active = true
      event-processing-active = ${?EVENT_PROCESSING_ACTIVE}

      sync {
        things {
          active = true
          active = ${?THINGS_SYNCHRONIZATION_ACTIVE}
          start-offset = 30m
          start-offset = ${?THINGS_SYNCHRONIZATION_START_OFFSET}
          initial-start-offset = 120m
          initial-start-offset = ${?THINGS_SYNCHRONIZATION_INITIAL_START_OFFSET}
          stream-interval = 5m
          stream-interval = ${?THINGS_SYNCHRONIZATION_STREAM_INTERVAL}
          outdated-warning-offset = 3h
          outdated-warning-offset = ${?THINGS_SYNCHRONIZATION_OUTDATED_WARNING_OFFSET}
          outdated-error-offset = 4h
          outdated-error-offset = ${?THINGS_SYNCHRONIZATION_OUTDATED_ERROR_OFFSET}
          max-idle-time = 5m
          max-idle-time = ${?THINGS_SYNCHRONIZATION_MAX_IDLE_TIME}
          streaming-actor-timeout = 10m
          streaming-actor-timeout = ${?THINGS_SYNCHRONIZATION_STREAMING_ACTOR_TIMEOUT}
          elements-streamed-per-batch = 10
          elements-streamed-per-batch = ${?THINGS_SYNCHRONIZATION_ELEMENTS_PER_SECOND}
          minimal-delay-between-streams = 5s
          minimal-delay-between-streams = ${?THINGS_SYNCHRONIZATION_MINIMAL_DELAY_BETWEEN_STREAMS}
        }

        policies {
          active = true
          active = ${?POLICIES_SYNCHRONIZATION_ACTIVE}
          start-offset = 30m
          start-offset = ${?POLICIES_SYNCHRONIZATION_START_OFFSET}
          initial-start-offset = 120m
          initial-start-offset = ${?POLICIES_SYNCHRONIZATION_INITIAL_START_OFFSET}
          stream-interval = 6m
          stream-interval = ${?POLICIES_SYNCHRONIZATION_STREAM_INTERVAL}
          outdated-warning-offset = 3h
          outdated-warning-offset = ${?POLICIES_SYNCHRONIZATION_OUTDATED_WARNING_OFFSET}
          outdated-error-offset = 4h
          outdated-error-offset = ${?POLICIES_SYNCHRONIZATION_OUTDATED_ERROR_OFFSET}
          max-idle-time = 5m
          max-idle-time = ${?POLICIES_SYNCHRONIZATION_MAX_IDLE_TIME}
          // streaming actor timeout is set generouslyl because each policy tag may trigger updates at many things.
          streaming-actor-timeout = 30m
          streaming-actor-timeout = ${?POLICIES_SYNCHRONIZATION_STREAMING_ACTOR_TIMEOUT}
          elements-streamed-per-batch = 1
          elements-streamed-per-batch = ${?POLICIES_SYNCHRONIZATION_ELEMENTS_PER_SECOND}
          minimal-delay-between-streams = 5s
          minimal-delay-between-streams = ${?POLICIES_SYNCHRONIZATION_MINIMAL_DELAY_BETWEEN_STREAMS}
        }
      }

      stream {
        // arrays bigger than this are not indexed
        max-array-size = 0
        max-array-size = ${?THINGS_SEARCH_UPDATER_MAX_ARRAY_SIZE}

        // minimum delay between event dumps
        write-interval = 1s
        write-interval = ${?THINGS_SEARCH_UPDATER_STREAM_WRITE_INTERVAL}

        // timeout for messages to Things-shard
        ask-timeout = 30s
        ask-timeout = ${?THINGS_SEARCH_UPDATER_STREAM_ASK_TIMEOUT}

        // retrieval of things and policy-enforcers
        retrieval {
          // upper bound of parallel SudoRetrieveThing commands (by extension, parallel loads of policy enforcer cache)
          parallelism = 25
          parallelism = ${?THINGS_SEARCH_UPDATER_STREAM_PARALLELISM}

          // back-offs in case of failure
          exponential-backoff {
            min = 1s
            max = 2m
            random-factor = 2.0
          }
        }

        // writing into the persistence
        persistence {
          // how many bulk writes to request in parallel; must be a power of 2
          parallelism = 1

          // how many write operations to perform in one bulk
          max-bulk-size = 250
          max-bulk-size = ${?MAX_BULK_SIZE}

          // how long to wait
          write-interval = 100ms
          write-interval = ${?THINGS_SEARCH_UPDATER_PERSISTENCE_WRITE_INTERVAL}

          // backoffs in case of failure
          exponential-backoff {
            min = 1s
            max = 2m
            random-factor = 2.0
          }
        }

        cache {
          // name of the dispatcher to run async cache loaders, which do not block threads
          dispatcher = "policy-enforcer-cache-dispatcher"

          // delay before retrying a cache query if the cached value is out of date
          retry-delay = 1s
          retry-delay = ${?THINGS_SEARCH_UPDATER_STREAM_CACHE_RETRY_DELAY}

          # how many enforcers to cache
          maximum-size = 20000
          maximum-size = ${?THINGS_SEARCH_UPDATER_STREAM_CACHE_SIZE}

          # lifetime of a cached enforcer. set very high because entries are invalidated lazily
          expire-after-write = 2h
          expire-after-write = ${?THINGS_SEARCH_UPDATER_STREAM_CACHE_EXPIRY}

          expire-after-access = 30m
          expire-after-access = ${?THINGS_SEARCH_UPDATER_STREAM_CACHE_EXPIRY_AFTER_ACCESS}
        }
      }
    }
  }

}

akka {
  cluster {
    sharding {
      role = "things-search"
    }

    roles = [
      "things-search",
      "blocked-namespaces-aware"
    ]
  }
}

search-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 4
    parallelism-factor = 3.0
    parallelism-max = 32
    parallelism-max = ${?DEFAULT_DISPATCHER_PARALLELISM_MAX}
  }
  throughput = 5
}

blocked-namespaces-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 4
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 3.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 32
    parallelism-max = ${?DEFAULT_DISPATCHER_PARALLELISM_MAX}
  }
  throughput = 5
}

policy-enforcer-cache-dispatcher {
  type = "Dispatcher"
  executor = "thread-pool-executor"
  thread-pool-executor {
    keep-alive-time = 60s
    fixed-pool-size = off
    max-pool-size-max = 256
    max-pool-size-max = ${?CACHE_DISPATCHER_POOL_SIZE_MAX}
    max-pool-size-max = ${?POLICY_ENFORCER_CACHE_DISPATCHER_POOL_SIZE_MAX}
  }
}
