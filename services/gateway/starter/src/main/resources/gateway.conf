ditto {
  mapping-strategy.implementation = "org.eclipse.ditto.services.gateway.starter.service.util.GatewayMappingStrategy"

  cluster {
    become-leader = false
    instance-index = ${?INSTANCE_INDEX}
  }

  services-utils-config.mongodb.options {
    ssl = false
    w = 1
  }

  gateway {
    http {
      # InetAddress.getLocalHost.getHostAddress is used if empty
      hostname = ""
      hostname = ${?HOSTNAME}
      hostname = ${?BIND_HOSTNAME}
      port = 8080
      port = ${?HTTP_PORT}
      port = ${?PORT}

      schema-versions = [1, 2]
      # override schema-versions via system properties, e.g.: -Dditto.gateway.http.schema-versions.0=1 -Dditto.gateway.http.schema-versions.1=2
    }

    cluster {
      # as a rule of thumb: should be factor ten of the amount of cluster nodes available
      # be aware that it must be the same as for all other services (e.g. search-updater)
      number-of-shards = 30
      number-of-shards = ${?CLUSTER_NUMBER_OF_SHARDS}

      # enables the majority check that solves network partitions automatically
      majority-check.enabled = false
      majority-check.enabled = ${?CLUSTER_MAJORITY_CHECK_ENABLED}

      # the delay after which the cluster majority is checked
      majority-check.delay = 30s
      majority-check.delay = ${?CLUSTER_MAJORITY_CHECK_DELAY}
    }

    websocket {
      # the max queue size of how many inflight Commands a single Websocket client can have
      subscriber.backpressure-queue-size = 100
      subscriber.backpressure-queue-size = ${?WS_SUBSCRIBER_BACKPRESSURE}

      # the max buffer size of how many outstanding CommandResponses and Events a single Websocket client can have
      # additionally CommandResponses and Events are dropped if this size is reached
      publisher.backpressure-buffer-size = 200
      publisher.backpressure-buffer-size = ${?WS_PUBLISHER_BACKPRESSURE}
    }

    enforcer {
      # the interval of how long to keep an "inactive" enforcer in memory:
      cache.interval = 2h
      cache.interval = ${?ENFORCER_CACHE_INTERVAL} # may be overridden with this environment variable

      # the interval of when to manually sync the underlying permission structure (policy or acl) of the enforcer
      sync.interval = 15m
      sync.interval = ${?ENFORCER_SYNC_INTERVAL} # may be overridden with this environment variable

      # the internal timeout when retrieving the Policy or the ACL or when waiting for a CommandResponse
      internal.ask.timeout = 5s
      internal.ask.timeout = ${?ENFORCER_INTERNAL_ASK_TIMEOUT} # may be overridden with this environment variable
    }

    message {
      default-timeout = 10s
      max-timeout = 1m
    }

    claim-message {
      default-timeout = 1m
      max-timeout = 10m
    }

    dns {
      # DNS server to use for looking up services
      address = none
      address = ${?DNS_SERVER} # may be overridden with this environment variable
    }

    authentication {
      # configures HTTP for different authentication mechanisms: IM3, JWT (e.g. Google), ...
      http {
        # proxy config
        proxy {
          enabled = false
          enabled = ${?AUTH_HTTP_PROXY_ENABLED}

          host = ${?AUTH_HTTP_PROXY_HOST}
          port = ${?AUTH_HTTP_PROXY_PORT}
          username = ${?AUTH_HTTP_PROXY_USERNAME}
          password = ${?AUTH_HTTP_PROXY_PASSWORD}
        }
      }

      dummy {
        # enable/disable dummy authentication (for dev purposes)
        enabled = false
        enabled = ${?ENABLE_DUMMY_AUTH}
      }
    }

    health-check {
      enabled = true
      enabled = ${?HEALTH_CHECK_ENABLED} # may be overridden with this environment variable
      interval = 60s
      service.timeout = 10s
      service.timeout = ${?HEALTH_CHECK_SERVICE_TIMEOUT} # may be overridden with this environment variable

      persistence {
        enabled = true
        enabled = ${?HEALTH_CHECK_PERSISTENCE_ENABLED} # may be overridden with this environment variable
        timeout = 60s
      }

      cluster-roles = {
        enabled = true
        enabled = ${?HEALTH_CHECK_ROLES_ENABLED} # may be overridden with this environment variable

        expected = [
          "policies",
          "things",
          "things-search",
          "gateway"
        ]
      }
    }

    devops {
      securestatus = true
      securestatus = ${?DEVOPS_SECURE_STATUS}
    }

    public-health {
      secure = true
      secure = ${?GATEWAY_PUBLIC_HEALTH_SECURE}
      cache-timeout = 20s
      cache-timeout = ${?GATEWAY_STATUS_HEALTH_EXTERNAL_TIMEOUT}
    }

    statsd {
      hostname = ${?STATSD_PORT_8125_UDP_ADDR}
      port = ${?STATSD_PORT_8125_UDP_PORT}
    }

    forcehttps = false
    forcehttps = ${?FORCE_HTTPS}

    enablecors = false
    forcehttps = ${?ENABLE_CORS}

    redirect-to-https = false
    redirect-to-https = ${?REDIRECT_TO_HTTPS}
    redirect-to-https-blacklist-pattern = "/api.*|/ws.*|/status.*"

    cache {
      publickeys {
        maxentries = 32
        expiry = 60m
      }
    }

    policy-enforcer {
      # the interval of how long to keep an "inactive" PolicyEnforcer in memory:
      cache.interval = 2h
      cache.interval = ${?POLICY_ENFORCER_CACHE_INTERVAL} # may be overridden with this environment variable

      # the interval of when to manually sync the underlying Policy of the PolicyEnforcer
      sync.interval = 15m
      sync.interval = ${?POLICY_ENFORCER_SYNC_INTERVAL} # may be overridden with this environment variable

      # the internal timeout when retrieving the Policy or the ACL or when waiting for a CommandResponse
      internal.ask.timeout = 5s
      internal.ask.timeout = ${?POLICY_ENFORCER_INTERNAL_ASK_TIMEOUT} # may be overridden with this environment variable
    }
  }
}

secrets {
  devops_password = "foobar"
  devops_password = ${?DEVOPS_PASSWORD}

  public_health_password = "healthUserPw1!"
  public_health_password = ${?PUBLIC_HEALTH_PASSWORD}
}

akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "DEBUG"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  # for log messages during the actor system is starting up and shutting down:
  stdout-loglevel = "INFO"

  log-config-on-start = off

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
    enable-additional-serialization-bindings = on

    # this is only intended for testing.
    serialize-messages = off
    serialize-creators = off

    debug {
      lifecycle = on
    }

    deployment {
      /gatewayRoot/proxy {
        router = round-robin-pool
        # nr-of-instances = 5
        resizer {
          lower-bound = 5
          upper-bound = 100
          messages-per-resize = 50
        }
      }
      /gatewayRoot/proxy/aggregator {
        router = round-robin-pool
        # nr-of-instances = 5
        resizer {
          lower-bound = 5
          upper-bound = 100
          messages-per-resize = 50
        }
      }
    }

    serializers {
      json = "org.eclipse.ditto.services.utils.cluster.JsonifiableSerializer"
    }

    serialization-bindings {
      #"java.io.Serializable" = none # must not be set in order to get akka.cluster.sharding.ShardRegion$GetShardRegionStats$ serialized
      # Serialize Jsonifiable events with custom JSON serializer:
      "org.eclipse.ditto.model.base.json.Jsonifiable" = json
      "org.eclipse.ditto.model.base.exceptions.DittoRuntimeException" = json
    }
  }

  extensions = [
    "akka.cluster.pubsub.DistributedPubSub",
    "akka.cluster.ddata.DistributedData"
  ]

  remote {
    log-remote-lifecycle-events = on
    netty.tcp {
      # InetAddress.getLocalHost.getHostAddress is used if empty
      hostname = ""
      hostname = ${?TCP_HOSTNAME}
      port = 2551
      port = ${?TCP_PORT}

      bind-hostname = ${?BIND_HOSTNAME}
      bind-port = ${?BIND_TCP_PORT}

      # maximum-frame-size = 128000b # this is the default
      maximum-frame-size = 10485760b # 10MB - things could get that big
      # send-buffer-size = 256000b # this is the default
      send-buffer-size = 20971520b # 20MB
      # receive-buffer-size = 256000b # this is the default
      receive-buffer-size = 20971520b # 20MB
    }
    watch-failure-detector.threshold = 12 # default 10
  }

  cluster {
    # Disable legacy metrics in akka-cluster.
    metrics.enabled = off

    # enable weakly up feature to allow members to join even if some members are unreachable
    allow-weakly-up-members = on

    # as fallback:
    # seed-nodes = ["akka.tcp://ditto-cluster@"${akka.remote.netty.tcp.hostname}":"${akka.remote.netty.tcp.port}]
    # seed-nodes = [ ] # otherwise they are dynamically set

    sharding {
      state-store-mode = ddata
      use-dispatcher = "sharding-dispatcher"

      role = "gateway"
    }

    roles = [
      "gateway",
      "thing-cache-aware",
      "policy-cache-aware"
    ]
  }

  http {

    server {
      server-header = "" # default: akka-http/${akka.http.version}
      request-timeout = 60s # default: 20 s
      request-timeout = ${?REQUEST_TIMEOUT}
      max-connections = 4096 # default: 1024
      backlog = 100 # default: 100
      raw-request-uri-header = on # default: off

      parsing {
        max-uri-length = 8k # default: 2k
        max-content-length = 10m # default: 8m
        illegal-header-warnings = off # default: on
        error-logging-verbosity = simple # default: full
      }
    }

    host-connection-pool {
      # The maximum number of open requests accepted into the pool across all
      # materializations of any of its client flows.
      # Protects against (accidentally) overloading a single pool with too many client flow materializations.
      # Note that with N concurrent materializations the max number of open request in the pool
      # will never exceed N * max-connections * pipelining-limit.
      # Must be a power of 2 and > 0!
      max-open-requests = 1024 # default: 32

      # The time after which an idle connection pool (without pending requests)
      # will automatically terminate itself. Set to `infinite` to completely disable idle timeouts.
      idle-timeout = 60s # default: 30s
    }
  }
}

rabbit-stats-bounded-mailbox {
  mailbox-type = "akka.dispatch.BoundedMailbox"
  mailbox-capacity = 10
  mailbox-push-timeout-time = 0s
}

sharding-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 3.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 128
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 5 # default is 5
}

aggregator-internal-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 4
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 2.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 64
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 5
}

blocking-dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    // or in Akka 2.4.2+
    fixed-pool-size = 16
  }
  throughput = 100
}

enforcer-lookup-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 4
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 2.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 64
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 5
}
