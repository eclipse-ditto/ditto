ditto {
  mapping-strategy.implementation = "org.eclipse.ditto.services.models.policies.PoliciesMappingStrategy"

  cluster {
    become-leader = false
    instance-index = ${?INSTANCE_INDEX}
  }

  services-utils-config.mongodb.options {
    ssl = false
    w = 1
  }

  policies {
    http {
      # InetAddress.getLocalHost.getHostAddress is used if empty
      hostname = ""
      hostname = ${?HOSTNAME}
      hostname = ${?BIND_HOSTNAME}
      port = 8080
      port = ${?HTTP_PORT}
      port = ${?PORT}
    }

    cluster {
      # as a rule of thumb: should be factor ten of the amount of cluster nodes available
      number-of-shards = 30
      number-of-shards = ${?CLUSTER_NUMBER_OF_SHARDS}

      # enables the majority check that solves network partitions automatically
      majority-check.enabled = false
      majority-check.enabled = ${?CLUSTER_MAJORITY_CHECK_ENABLED}

      # the delay after which the cluster majority is checked
      majority-check.delay = 30s
      majority-check.delay = ${?CLUSTER_MAJORITY_CHECK_DELAY}
    }

    policy {
      # the interval of how long to keep an "inactive" Policy in memory:
      activity.check.interval = 2h
      activity.check.interval = ${?POLICY_ACTIVITY_CHECK_INTERVAL} # may be overridden with this environment variable

      # the interval to check for modifications and send out an event if modified:
      modification.check.interval = 10s
      modification.check.interval = ${?POLICY_MODIFICATION_CHECK_INTERVAL} # may be overridden with this environment variable

      snapshot {
        # the interval when to do snapshot for a Policy which had changes to it
        interval = 15m
        interval = ${?POLICY_SNAPSHOT_INTERVAL} # may be overridden with this environment variable

        # the threshold after how many changes to a Policy to do a snapshot
        threshold = 500
        threshold = ${?POLICY_SNAPSHOT_THRESHOLD} # may be overridden with this environment variable

        # delete old Snapshot when taking a Snapshot
        delete-old = false
        delete-old = ${?POLICY_SNAPSHOT_DELETE_OLD} # may be overridden with this environment variable
      }

      events {
        # delete old Events when taking a Snapshot
        delete-old = false
        delete-old = ${?POLICY_EVENTS_DELETE_OLD} # may be overridden with this environment variable
      }

      supervisor {
        exponential-backoff {
          min = 1s
          max = 10s
          random-factor = 0.2
        }
      }
    }

    health-check {
      enabled = true
      enabled = ${?HEALTH_CHECK_ENABLED} # may be overridden with this environment variable
      interval = 60s

      persistence {
        enabled = true
        enabled = ${?HEALTH_CHECK_PERSISTENCE_ENABLED} # may be overridden with this environment variable
        timeout = 60s
      }
    }

    statsd {
      hostname = ${?STATSD_PORT_8125_UDP_ADDR}
      port = ${?STATSD_PORT_8125_UDP_PORT}
    }
  }
}

akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "DEBUG"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  # for log messages during the actor system is starting up and shutting down:
  stdout-loglevel = "INFO"

  log-config-on-start = off

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
    enable-additional-serialization-bindings = on

    # this is only intended for testing.
    serialize-messages = off
    serialize-creators = off

    debug {
      lifecycle = on
    }

    serializers {
      json = "org.eclipse.ditto.services.utils.cluster.JsonifiableSerializer"
    }

    serialization-bindings {
      # "java.io.Serializable" = none # must not be set in order to get akka.cluster.sharding.ShardRegion$GetShardRegionStats$ serialized
      # Serialize Jsonifiable events with custom JSON serializer:
      "org.eclipse.ditto.model.base.json.Jsonifiable" = json
      "org.eclipse.ditto.model.base.exceptions.DittoRuntimeException" = json
    }
  }

  extensions = [
    "akka.cluster.pubsub.DistributedPubSub",
    "akka.cluster.ddata.DistributedData"
  ]

  remote {
    log-remote-lifecycle-events = on
    netty.tcp {
      # InetAddress.getLocalHost.getHostAddress is used if empty
      hostname = ""
      hostname = ${?TCP_HOSTNAME}
      port = 2551
      port = ${?TCP_PORT}

      bind-hostname = ${?BIND_HOSTNAME}
      bind-port = ${?BIND_TCP_PORT}

      # maximum-frame-size = 128000b # this is the default
      maximum-frame-size = 10485760b # 10MB - policies could get that big
      # send-buffer-size = 256000b # this is the default
      send-buffer-size = 20971520b # 20MB
      # receive-buffer-size = 256000b # this is the default
      receive-buffer-size = 20971520b # 20MB
    }
    watch-failure-detector.threshold = 12 # default 10
  }

  cluster {
    # Disable legacy metrics in akka-cluster.
    metrics.enabled = off

    # enable weakly up feature to allow members to join even if some members are unreachable
    allow-weakly-up-members = on

    # as fallback:
    # seed-nodes = ["akka.tcp://ditto-cluster@"${akka.remote.netty.tcp.hostname}":"${akka.remote.netty.tcp.port}]
    # seed-nodes = [ ] # otherwise they are dynamically set

    sharding {
      state-store-mode = ddata
      use-dispatcher = "sharding-dispatcher"

      role = "policies"
    }

    roles = [
      "policies",
      "policy-cache-aware"
    ]
  }

  persistence {
    journal.auto-start-journals = [
      "akka-contrib-mongodb-persistence-policies-journal"
    ]
    snapshot-store.auto-start-snapshot-stores = [
      "akka-contrib-mongodb-persistence-policies-snapshots"
    ]
  }
}

akka.contrib.persistence.mongodb.mongo {
  driver = "akka.contrib.persistence.mongodb.DittoCasbahPersistenceExtension"

  # Write concerns are one of: ErrorsIgnored, Unacknowledged, Acknowledged, Journaled, ReplicaAcknowledged
  journal-write-concern = "Acknowledged" # By default was: "Journaled"
  journal-wtimeout = 10000
  journal-fsync = false

  snaps-write-concern = "Acknowledged" # By default was: "Journaled"
  snaps-wtimeout = 5000
  snaps-fsync = false

  realtime-enable-persistence = false
}

akka-contrib-mongodb-persistence-policies-journal {
  class = "akka.contrib.persistence.mongodb.MongoJournal"
  plugin-dispatcher = "policy-persistence-dispatcher"

  circuit-breaker {
    max-failures = 5 # if an exception during persisting an event/snapshot occurs this often -- a successful write resets the counter
    max-failures = ${?BREAKER_MAXTRIES}
    call-timeout = 5s # MongoDB Timeouts causing the circuitBreaker to open
    call-timeout = ${?BREAKER_TIMEOUT}
    reset-timeout = 3s # after this time in "Open" state, the cicuitBreaker is "Half-opened" again
    reset-timeout = ${?BREAKER_RESET}
  }

  overrides {
    journal-collection = "policies_journal"
    journal-index = "policies_journal_index"

    realtime-collection = "policies_realtime"
    metadata-collection = "policies_metadata"
  }

  event-adapters {
    mongodbobject = "org.eclipse.ditto.services.policies.persistence.serializer.MongoPolicyEventAdapter"
  }

  event-adapter-bindings {
    "org.eclipse.ditto.signals.events.policies.PolicyEvent" = mongodbobject
    "com.mongodb.DBObject" = mongodbobject
  }
}

akka-contrib-mongodb-persistence-policies-snapshots {
  class = "akka.contrib.persistence.mongodb.MongoSnapshots"
  plugin-dispatcher = "policy-persistence-dispatcher"

  circuit-breaker {
    max-failures = 5 # if an exception during persisting an event/snapshot occurs this often -- a successful write resets the counter
    max-failures = ${?SNAPSHOT_BREAKER_MAXTRIES}
    call-timeout = 10s # MongoDB Timeouts causing the circuitBreaker to open
    call-timeout = ${?SNAPSHOT_BREAKER_TIMEOUT}
    reset-timeout = 6s # after this time in "Open" state, the cicuitBreaker is "Half-opened" again
    reset-timeout = ${?SNAPSHOT_BREAKER_RESET}
  }

  overrides {
    snaps-collection = "policies_snaps"
    snaps-index = "policies_snaps_index"
  }
}

# journal & snapshot collections for akka.contrib.persistence.mongodb.DittoMongoReadJournal
ditto-akka-persistence-mongo-readjournal {
  # Class name of the plugin.
  class = "akka.contrib.persistence.mongodb.DittoMongoReadJournal"

  overrides {
    journal-collection = "policies_journal"
    journal-index = "policies_journal_index"

    realtime-collection = "policies_realtime"
    metadata-collection = "policies_metadata"

    snaps-collection = "policies_snaps"
    snaps-index = "policies_snaps_index"
  }

  tags.publishing {
    # if the publishing of thing tags should be enabled or not
    enabled = true
    enabled = ${?POLICY_TAGS_PUBLISHING_ENABLED}

    # the pause before the first and between two runs over the whole collection of tags
    pause = 1m
    pause = ${?POLICY_TAGS_PUBLISHING_PAUSE}

    # the offset after which thing tags without changes will be published
    offset = 60m
    offset = ${?POLICY_TAGS_PUBLISHING_OFFSET}

    # how many elements in an interval will be published at most
    elements = 10
    elements = ${?POLICY_TAGS_PUBLISHING_ELEMENTS}

    # the interval to publish the given amount of elements
    interval = 1m
    interval = ${?POLICY_TAGS_PUBLISHING_INTERVAL}
  }
}

policy-persistence-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # which mailbox to use
  mailbox-type = "org.eclipse.ditto.services.policies.persistence.actors.policy.PolicyPersistenceActorMailbox"
  mailbox-capacity = 100
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 3.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 128
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 5 # default is 5
}

sharding-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 3.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 128
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 5 # default is 5
}
